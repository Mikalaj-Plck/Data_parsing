{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCVlriXWnU6nUG/8FrfWYL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mikalaj-Plck/Data_parsing/blob/main/HTML_Beautiful_soup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "\n",
        "def clean_price(price_str):\n",
        "    # Очистка строки цены от лишних символов и кодировок\n",
        "    # Удаляем специальные символы и пробелы\n",
        "    price_str = re.sub(r'[^\\d.,]', '', price_str)\n",
        "\n",
        "    # Заменяем запятую на точку, если используется запятая\n",
        "    price_str = price_str.replace(',', '.')\n",
        "\n",
        "    try:\n",
        "        return float(price_str)\n",
        "    except ValueError:\n",
        "        print(f\"Не удалось преобразовать цену: {price_str}\")\n",
        "        return 0.0\n",
        "\n",
        "def scrape_books():\n",
        "    base_url = \"http://books.toscrape.com/catalogue/\"\n",
        "    index_url = \"http://books.toscrape.com/index.html\"\n",
        "    all_books = []\n",
        "\n",
        "    # Получаем список категорий\n",
        "    try:\n",
        "        response = requests.get(index_url)\n",
        "        response.encoding = 'utf-8'  # Явно устанавливаем кодировку\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Находим ссылки на категории\n",
        "        category_links = [\n",
        "            \"http://books.toscrape.com/\" + link['href']\n",
        "            for link in soup.select('ul.nav-list ul li a')\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при получении списка категорий: {e}\")\n",
        "        return []\n",
        "\n",
        "    # Обработка каждой категории\n",
        "    for category_url in category_links:\n",
        "        page_num = 1\n",
        "        while True:\n",
        "            try:\n",
        "                # Первая страница категории\n",
        "                if page_num == 1:\n",
        "                    current_url = category_url\n",
        "                else:\n",
        "                    # Для последующих страниц\n",
        "                    current_url = category_url.replace('index.html', f'page-{page_num}.html')\n",
        "\n",
        "                # Загрузка страницы\n",
        "                response = requests.get(current_url)\n",
        "                response.encoding = 'utf-8'  # Явно устанавливаем кодировку\n",
        "\n",
        "                # Если страница не найдена, переходим к следующей категории\n",
        "                if response.status_code != 200:\n",
        "                    break\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Находим все книги на странице\n",
        "                books = soup.select('article.product_pod')\n",
        "\n",
        "                # Если книг нет, прекращаем обработку категории\n",
        "                if not books:\n",
        "                    break\n",
        "\n",
        "                # Обработка каждой книги\n",
        "                for book in books:\n",
        "                    try:\n",
        "                        # Получаем ссылку на книгу\n",
        "                        book_relative_link = book.select_one('h3 a')['href']\n",
        "                        book_link = base_url + book_relative_link.replace('../', '')\n",
        "\n",
        "                        # Загружаем страницу книги\n",
        "                        book_response = requests.get(book_link)\n",
        "                        book_response.encoding = 'utf-8'  # Явно устанавливаем кодировку\n",
        "                        book_soup = BeautifulSoup(book_response.text, 'html.parser')\n",
        "\n",
        "                        # Извлекаем информацию\n",
        "                        title = book.select_one('h3 a')['title']\n",
        "\n",
        "                        # Очистка цены с использованием специальной функции\n",
        "                        price_elem = book.select_one('p.price_color')\n",
        "                        price = clean_price(price_elem.text) if price_elem else 0.0\n",
        "\n",
        "                        # Количество в наличии\n",
        "                        stock_text = book.select_one('p.instock').text.strip()\n",
        "                        stock_match = re.search(r'\\((\\d+) available\\)', stock_text)\n",
        "                        stock = int(stock_match.group(1)) if stock_match else 0\n",
        "\n",
        "                        # Описание\n",
        "                        description_elem = book_soup.select_one('#product_description + p')\n",
        "                        description = description_elem.text.strip() if description_elem else \"Нет описания\"\n",
        "\n",
        "                        # Сохраняем информацию о книге\n",
        "                        book_info = {\n",
        "                            \"title\": title,\n",
        "                            \"price\": price,\n",
        "                            \"stock\": stock,\n",
        "                            \"description\": description\n",
        "                        }\n",
        "\n",
        "                        all_books.append(book_info)\n",
        "\n",
        "                    except Exception as book_error:\n",
        "                        print(f\"Ошибка при обработке книги: {book_error}\")\n",
        "\n",
        "                # Переход к следующей странице\n",
        "                page_num += 1\n",
        "\n",
        "                # Небольшая задержка\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            except Exception as category_error:\n",
        "                print(f\"Ошибка при обработке категории: {category_error}\")\n",
        "                break\n",
        "\n",
        "    # Сохраняем результаты\n",
        "    with open('books_data.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_books, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Скрейпинг завершен. Всего книг: {len(all_books)}\")\n",
        "    return all_books\n",
        "\n",
        "# Запуск парсера\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_books()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XID4VmtJGtVb",
        "outputId": "a46e07e2-2df1-47eb-b6ff-05aa695f8934"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Скрейпинг завершен. Всего книг: 1000\n"
          ]
        }
      ]
    }
  ]
}